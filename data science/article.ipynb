{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0caf7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<!DOCTYPE html><html lang=\"en-GB\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width\"/><title>BBC News - Breaking news, video and the latest top stories from the U.S. and around the world</title><meta name=\"page.section\" content=\"News\"/><meta property=\"og:title\" content=\"BBC News - Breaking news, video and the latest top stories from the U.S. and around the world\"/><meta name=\"twitter:title\" content=\"BBC News - Breaking news, video and the latest top stories from the U.S. and around the world\"/><meta name=\"description\" content=\"Visit BBC News for the latest news, breaking news, video, audio and analysis. BBC News provides trusted World, U.S. and U.K. news as well as local and regional perspectives. Also entertainment, climate, business, science, technology and health news.\"/><meta property=\"og:description\" content=\"Visit BBC News for the latest news, breaking news, video, audio and analysis. BBC News provides trusted World, U.S. and U.K. news as well as loca\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://www.bbc.com/news\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9661fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6554b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(\"h2\",class_=\"sc-fa814188-3 iCfgww\")\n",
    "abstracts = soup.find_all(\"p\",class_=\"sc-f61c18b2-0 jktPA-D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dedafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Watch: Storm chasers fly inside the eye of Hur...   \n",
      "1  Skip short strolls â€“ a longer daily walk is be...   \n",
      "2  Man pleads guilty to killing Japan's former PM...   \n",
      "3  Sudan's army loses key city of el-Fasher to pa...   \n",
      "4  Watch: Storm chasers fly inside the eye of Hur...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Footage taken by a US Air Force Reserve crew k...  \n",
      "1  Walking for at least 15 minutes without stoppi...  \n",
      "2  Footage taken by a US Air Force Reserve crew k...  \n",
      "3  Donald Trump is in Japan and will fly to South...  \n",
      "4  The trio were at Andrew's mansion in 2006 as p...  \n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "product_data =[]\n",
    "for t, a in zip(titles, abstracts):\n",
    "    product_data.append({\"Title\": t.get_text(strip=True),\n",
    "    \"abstract\": a.get_text(strip=True)})\n",
    "df= pd.DataFrame(product_data)\n",
    "print(df.head())\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbc3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Saved admininfo_all_products.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "all_products =[]\n",
    "page =1\n",
    "while page<5:\n",
    "    print(\"Scraping page\", page)\n",
    "    url = url.format(page)\n",
    "    response = requests.get(url )\n",
    "    if response.status_code != 200:\n",
    "        print(\"No more pages.\")\n",
    "        break\n",
    "    soup= BeautifulSoup(response.text,\"html.parser\")\n",
    "    titles = soup.find_all(\"h2\",class_=\"sc-fa814188-3 iCfgww\")\n",
    "    abstracts = soup.find_all(\"p\",class_=\"sc-f61c18b2-0 jktPA-D\")    \n",
    "    if not titles or not abstracts:\n",
    "        break\n",
    "    for t,a in zip(titles, abstracts):\n",
    "        all_products.append({\"Title\": t.get_text(strip=True),\n",
    "        \"abstract\": a.get_text(strip=True)})\n",
    "    page += 1\n",
    "df = pd.DataFrame(all_products)\n",
    "df.to_csv(\"admininfo_all_products.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Saved admininfo_all_products.csv successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
